<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-Bellman:1957">
<p>Bellman, Richard. 1957. <em>Dynamic Programming</em>. 1st ed. Princeton, NJ, USA: Princeton University Press. <a href="http://books.google.com/books?id=fyVtp3EMxasC&amp;pg=PR5&amp;dq=dynamic+programming+richard+e+bellman&amp;client=firefox-a#v=onepage&amp;q=dynamic%20programming%20richard%20e%20bellman&amp;f=false">http://books.google.com/books?id=fyVtp3EMxasC&amp;pg=PR5&amp;dq=dynamic+programming+richard+e+bellman&amp;client=firefox-a#v=onepage&amp;q=dynamic%20programming%20richard%20e%20bellman&amp;f=false</a>.</p>
</div>
<div id="ref-NIPS2014_5378">
<p>Besbes, Omar, Yonatan Gur, and Assaf Zeevi. 2014. “Stochastic Multi-Armed-Bandit Problem with Non-Stationary Rewards.” In <em>Advances in Neural Information Processing Systems 27</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, 199–207. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf" class="uri">http://papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf</a>.</p>
</div>
<div id="ref-Browne2012">
<p>Browne, Cb, and Edward Powley. 2012. “A survey of monte carlo tree search methods.” <em>Intelligence and AI</em> 4 (1): 1–49. doi:<a href="https://doi.org/10.1109/TCIAIG.2012.2186810">10.1109/TCIAIG.2012.2186810</a>.</p>
</div>
<div id="ref-DBLP:journals/ftml/BubeckC12">
<p>Bubeck, Sébastien, and Nicolò Cesa-Bianchi. 2012. “Regret Analysis of Stochastic and Nonstochastic Multi-Armed Bandit Problems.” <em>Foundations and Trends in Machine Learning</em> 5 (1): 1–122. doi:<a href="https://doi.org/10.1561/2200000024">10.1561/2200000024</a>.</p>
</div>
<div id="ref-NIPS2011_4321">
<p>Chapelle, Olivier, and Lihong Li. 2011. “An Empirical Evaluation of Thompson Sampling.” In <em>Advances in Neural Information Processing Systems 24</em>, edited by J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, 2249–57. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf" class="uri">http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf</a>.</p>
</div>
<div id="ref-DBLP:conf/nips/CombesMP17">
<p>Combes, Richard, Stefan Magureanu, and Alexandre Proutière. 2017. “Minimal Exploration in Structured Stochastic Bandits.” In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, ca, USA</em>, 1761–9. <a href="http://papers.nips.cc/paper/6773-minimal-exploration-in-structured-stochastic-bandits" class="uri">http://papers.nips.cc/paper/6773-minimal-exploration-in-structured-stochastic-bandits</a>.</p>
</div>
<div id="ref-DBLP:conf/aaai/DabneyRBM18">
<p>Dabney, Will, Mark Rowland, Marc G. Bellemare, and Rémi Munos. 2018. “Distributional Reinforcement Learning with Quantile Regression.” In <em>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (Aaai-18), the 30th Innovative Applications of Artificial Intelligence (Iaai-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (Eaai-18), New Orleans, Louisiana, Usa, February 2-7, 2018</em>, 2892–2901. <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17184" class="uri">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17184</a>.</p>
</div>
<div id="ref-LAI19854">
<p>Lai, T.L, and Herbert Robbins. 1985. “Asymptotically Efficient Adaptive Allocation Rules.” <em>Advances in Applied Mathematics</em> 6 (1): 4–22. doi:<a href="https://doi.org/https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>.</p>
</div>
<div id="ref-journals/corr/LillicrapHPHETS15">
<p>Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” <em>CoRR</em> abs/1509.02971. <a href="http://dblp.uni-trier.de/db/journals/corr/corr1509.html#LillicrapHPHETS15" class="uri">http://dblp.uni-trier.de/db/journals/corr/corr1509.html#LillicrapHPHETS15</a>.</p>
</div>
<div id="ref-pmlr-v48-mniha16">
<p>Mnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” In <em>Proceedings of the 33rd International Conference on Machine Learning</em>, edited by Maria Florina Balcan and Kilian Q. Weinberger, 48:1928–37. Proceedings of Machine Learning Research. New York, New York, USA: PMLR. <a href="http://proceedings.mlr.press/v48/mniha16.html" class="uri">http://proceedings.mlr.press/v48/mniha16.html</a>.</p>
</div>
<div id="ref-DBLP:journals/corr/MnihKSGAWR13">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning.” <em>CoRR</em> abs/1312.5602. <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-46647">
<p>Riquelme, Carlos, George Tucker, and Jasper Roland Snoek. 2018. “Deep Bayesian Bandits Showdown.” In. <a href="https://openreview.net/pdf?id=SyYe6k-CW" class="uri">https://openreview.net/pdf?id=SyYe6k-CW</a>.</p>
</div>
<div id="ref-DBLP:journals/pieee/ShahriariSWAF16">
<p>Shahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. 2016. “Taking the Human Out of the Loop: A Review of Bayesian Optimization.” <em>Proceedings of the IEEE</em> 104 (1): 148–75. doi:<a href="https://doi.org/10.1109/JPROC.2015.2494218">10.1109/JPROC.2015.2494218</a>.</p>
</div>
<div id="ref-DBLP:journals/corr/abs-1712-01815">
<p>Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.” <em>CoRR</em> abs/1712.01815. <a href="http://arxiv.org/abs/1712.01815" class="uri">http://arxiv.org/abs/1712.01815</a>.</p>
</div>
<div id="ref-Sutton1998">
<p>Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press. <a href="http://incompleteideas.net/book/the-book-2nd.html" class="uri">http://incompleteideas.net/book/the-book-2nd.html</a>.</p>
</div>
<div id="ref-Tesauro:1995:TDL:203330.203343">
<p>Tesauro, Gerald. 1995. “Temporal Difference Learning and Td-Gammon.” <em>Commun. ACM</em> 38 (3). New York, NY, USA: ACM: 58–68. doi:<a href="https://doi.org/10.1145/203330.203343">10.1145/203330.203343</a>.</p>
</div>
<div id="ref-10.2307/2332286">
<p>Thompson, William R. 1933. “On the Likelihood That One Unknown Probability Exceeds Another in View of the Evidence of Two Samples.” <em>Biometrika</em> 25 (3/4). [Oxford University Press, Biometrika Trust]: 285–94. <a href="http://www.jstor.org/stable/2332286" class="uri">http://www.jstor.org/stable/2332286</a>.</p>
</div>
<div id="ref-Watkins:1989">
<p>Watkins, Christopher John Cornish Hellaby. 1989. “Learning from Delayed Rewards.” PhD thesis, Cambridge, UK: King’s College. <a href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf" class="uri">http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf</a>.</p>
</div>
<div id="ref-Wu:2018:LCB:3209978.3210051">
<p>Wu, Qingyun, Naveen Iyer, and Hongning Wang. 2018. “Learning Contextual Bandits in a Non-Stationary Environment.” In <em>The 41st International Acm Sigir Conference on Research &amp;#38; Development in Information Retrieval</em>, 495–504. SIGIR ’18. New York, NY, USA: ACM. doi:<a href="https://doi.org/10.1145/3209978.3210051">10.1145/3209978.3210051</a>.</p>
</div>
</div>
</body>
</html>
